# -*- coding: utf-8 -*-
"""PersonaChat_preprocess.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/184JC4BlaUHwQP0MNEaK-F_SSdg39VcrR
"""

from google.colab import drive
drive.mount("/content/drive")

import pandas as pd
import re

pd.set_option('display.max_colwidth', None)

per_chat = pd.read_csv("/content/drive/MyDrive/602-PrinciplesofDS/Project/Synthetic-PersonaChat/New-Persona-New-Conversations.csv")
per_chat.head(10)

print(per_chat.iloc[:1,2])

per_chat.info()

per_chat["total_turns"] = per_chat['Best Generated Conversation'].str.count(r'\nUser',flags=re.IGNORECASE) + 1
per_chat.head()

per_chat[per_chat['total_turns']==38]

per_chat.iloc[10896,:]

# Example: Assuming final_df_filtered is your DataFrame
# Group by 'conversation_id' and find the maximum 'sub_conversation_id' for each conversation
#max_turns_per_conversation = per_chat.groupby('conversation_id').max()

# Count the occurrences of each maximum turn count
turn_count_summary = per_chat['total_turns'].value_counts().sort_index()

# Display the summary as a table
turn_count_table = pd.DataFrame({'Max Turns': turn_count_summary.index, 'Number of Conversations': turn_count_summary.values})
print(turn_count_table)



import re
import pandas as pd

def turn_splits(df, col):
    processed_df = []

    valid_users = ['User 1', 'User 2']

    for conv_id, row in df.iterrows():
        conversation = row[col].replace('\r\n', '\n').strip()
        lines = conversation.split('\n')

        for line in lines:
          #print(line)
          #line = re.sub(r'^[^\w]*|[^\w]*$', '', line).strip()
          line = re.sub(r'[^A-Za-z0-9\s.!?\':]', '', line).strip()
          if line == "":
              continue

          # Skip the line if it doesn't start with 'User 1' or 'User 2'
          if not any(line.startswith(user) for user in valid_users):
              continue

          #print(line)

          for user in valid_users:
              user_pattern = re.escape(user) + r'\s*:\s*'
              if re.match(user_pattern, line, re.IGNORECASE):
                  message = re.sub(user_pattern, '', line, flags=re.IGNORECASE).strip()
                  message = re.sub(r'[()\[\]{}]', '', message)
                  message = re.sub(r'[^\w\s.,!?]', '', message).strip()

                  processed_df.append({'conversation_id': conv_id,
                                        'user': user,
                                        'message': message})

    new_df = pd.DataFrame(processed_df)

    return new_df

per_chat_turned = turn_splits(per_chat,'Best Generated Conversation')
per_chat_turned.head(10)

per_chat_turned.head(105)

for conversation_id, group in per_chat_turned.iloc[:102,:].groupby('conversation_id'):
    conversation = group['message'].tolist()
    print(conversation)

per_chat_turned.head(20)

per_chat_turned.info()

per_chat_turned['user'].unique()

nturns=per_chat_turned['conversation_id'].value_counts()
nturns = pd.DataFrame(nturns)
nturns = nturns.sort_index(ascending=True)

nturns

differences = per_chat['total_turns'].ne(nturns['count'])

# Get the indices where values are different
different_indices = differences[differences].index

print("Indices where columns are different:", different_indices)



per_chat_turned['message'].nunique()

#per_chat_turned.to_csv('/content/drive/MyDrive/602-PrinciplesofDS/Project/Synthetic-PersonaChat/new_persona_turned.csv')

synth_tr = pd.read_csv("/content/drive/MyDrive/602-PrinciplesofDS/Project/Synthetic-PersonaChat/Synthetic-Persona-Chat_train.csv")
synth_tr.head()

synth_tr.shape

synth_tr_turned = turn_splits(synth_tr,'Best Generated Conversation')
synth_tr_turned.conversation_id.nunique()

synth_tr_turned.head()



max_turns_per_conversation.value_counts().sort_index()

synth_tr_turned.to_csv('/content/drive/MyDrive/602-PrinciplesofDS/Project/Synthetic-PersonaChat/synth_per_turned.csv')

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
import re

# Install and download NLTK resources if necessary
# nltk.download('punkt')

# Sample conversation
conversation = [
    "Hi! I'm [user 1's name].",
    "Hi [user 1's name], I'm [user 2's name].",
    "What do you do for fun?",
    "I like to play video games, go to the beach, and read.",
    "I like to play video games too! I'm not much of a reader, though.",
    "What video games do you like to play?",
    "I like to play a lot of different games, but I'm really into competitive online games right now.",
    "I'm not really into competitive games, I like to play more relaxing games.",
    "That's cool. What kind of relaxing games do you like to play?",
    "I like to play puzzle games, simulation games, and story-based games.",
    "I've never been much of a puzzle game person, but I do like simulation games and story-based games.",
    "Nice! What's your favorite simulation game?",
    "I like Stardew Valley a lot. It's a farming game, but it's also really relaxing and fun.",
    "I've heard good things about that game. I might have to check it out.",
    "You should! It's a lot of fun.",
    "Well, I'm glad we met. Maybe we can play some games together sometime.",
    "That would be fun!",
    "Great! I'll send you my Steam name.",
    "Ok, sounds good."
]

# Preprocess text: lowercasing and removing special characters
def preprocess(text):
    text = text.lower()
    text = re.sub(r'\W+', ' ', text)
    return text

# Preprocess the conversation
preprocessed_conversation = [preprocess(sentence) for sentence in conversation]

# Create TF-IDF vectors for each message
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(preprocessed_conversation)

# Corrected loop using tfidf_matrix.shape[0]

# Measure cosine similarity between consecutive messages
similarities = []
for i in range(tfidf_matrix.shape[0] - 1):  # Using shape[0] to get the number of messages
    similarity = cosine_similarity(tfidf_matrix[i], tfidf_matrix[i + 1])[0][0]
    similarities.append(similarity)

# Detect topic shifts where similarity is below a defined threshold
topic_shift_indices = []
threshold = 0.1
  # Define a threshold for topic shift (tune as needed)
for i, similarity in enumerate(similarities):
    if similarity < threshold:
        topic_shift_indices.append(i)

# Number of topic shifts
num_topic_shifts = len(topic_shift_indices)
num_topic_shifts

topic_shift_indices

# Extract the sentences where topic shifts occur
topic_shift_sentences = []
for index in topic_shift_indices:
    topic_shift_sentences.append((conversation[index], conversation[index + 1]))

# Display the sentences involved in topic shifts
topic_shift_sentences

from sentence_transformers import SentenceTransformer, util

# Example conversation
conversation = [
    "Hey how are you doing?",
    "Doing well, how about yourself?",
    "I'm doing great! I'm a fulltime student studying radiology at a local college and I also play in a band that my parents don't know about.",
    "That's awesome! I'm a big fan of punk music myself, and I like to run and do yoga in my spare time.",
    "Nice! I've been meaning to try yoga but I've never really had the time.",
    "It's really great for stress relief and it's a great way to get in shape.",
    "I'll have to try it sometime! What about your Impala?",
    "It's a 1964 Impala and it's in pretty good condition, but it needs a new paint job. I'm going to start working on that this weekend.",
    "That's awesome! I love classic cars.",
    "Thanks! It's a lot of work, but it's worth it in the end.",
    "I'm sure it will be!",
    "So what's your favorite thing about being a student?",
    "I love the freedom of being able to learn whatever I want.",
    "That's definitely the best part"
]

# Load Sentence-BERT model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Compute embeddings for each sentence
embeddings = model.encode(conversation, convert_to_tensor=True)

# Compute cosine similarity between consecutive sentences
similarities = util.pytorch_cos_sim(embeddings[:-1], embeddings[1:])

# Detect topic shifts by analyzing significant drops in similarity
threshold = 0.5  # Lower threshold to identify large semantic shifts
topic_shifts = []
for i, similarity in enumerate(similarities):
    if similarity < threshold:
        topic_shifts.append((conversation[i], conversation[i + 1]))

# Output topic shifts
topic_shifts

from sentence_transformers import SentenceTransformer, util

# Example conversation
# conversation = [
#     "Hey how are you doing?",
#     "Doing well, how about yourself?",
#     "I'm doing great! I'm a fulltime student studying radiology at a local college and I also play in a band that my parents don't know about.",
#     "That's awesome! I'm a big fan of punk music myself, and I like to run and do yoga in my spare time.",
#     "Nice! I've been meaning to try yoga but I've never really had the time.",
#     "It's really great for stress relief and it's a great way to get in shape.",
#     "I'll have to try it sometime! What about your Impala?",
#     "It's a 1964 Impala and it's in pretty good condition, but it needs a new paint job. I'm going to start working on that this weekend.",
#     "That's awesome! I love classic cars.",
#     "Thanks! It's a lot of work, but it's worth it in the end.",
#     "I'm sure it will be!",
#     "So what's your favorite thing about being a student?",
#     "I love the freedom of being able to learn whatever I want.",
#     "That's definitely the best part"
# ]

# conversation = [
#     "How is your day going?"
#     "Great! I am in the process of moving across the country for my new job as a firefighter.",
#     "Wow, that is really exciting! I am always impressed by firefighters.",
#     "Thanks, it is quite an exciting opportunity. I am a little nervous about moving away from my family and friends, but I am looking forward to the new adventure.",
#     "I can understand that. Moving to a new place can be daunting, but it can also be a lot of fun. I am sure you will make some great new friends and have a lot of new experiences.",
#     "I hope so! I am trying to stay positive and look forward to the future.",
#     "That is the best way to be. I am sure you will do great things.",
#     "Thanks, I appreciate that.",
#     "So, what are you looking forward to the most about your new job?",
#     "I am looking forward to helping people and making a difference in my community. I am also looking forward to learning new things and becoming a better firefighter.",
#     "That is great! I am sure you will do a lot of good work.",
#     "I hope so! I am excited to get started.",
# ]

# conversation = [
#     "Hi!",
#     "Hey there!",
#     "How are you doing today?",
#     "I'm doing well, thanks for asking! Just got home from a long day at work. How about you?",
#     "I'm doing alright. I'm trying to keep up with my studies while also taking care of my dog. It's a lot of work, but I'm managing.",
#     "I know what you mean! I have a son and I work full time, so I'm always busy. But it's worth it to spend time with the people I love.",
#     "That's true. I'm glad you have a good support system.",
#     "Me too! What are you studying?",
#     "I'm studying computer science. I'm hoping to get a job as a programmer after I graduate.",
#     "Oh, that's cool! I was a computer programmer before I started my own business.",
#     "Really? What kind of business do you have?",
#     "I own a gym. I train people to stay in peak physical condition.",
#     "That's awesome! I'm trying to do the same thing, but I'm not as disciplined as you are.",
#     "It's not easy, but it's worth it!\nUser 1: I'm sure it is. I'm glad you're happy with your life.",
#     "Thanks! I am. I have a great family and a great business. I can't complain!",
#     "That's good to hear! I'm happy for you.",
#     "Thanks"
# ]

# conversation = [
#     "Hi! I'm [user 1's name].",
#     "Hi [user 1's name], I'm [user 2's name].",
#     "What do you do for fun?",
#     "I like to play video games, go to the beach, and read.",
#     "I like to play video games too! I'm not much of a reader, though.",
#     "What video games do you like to play?",
#     "I like to play a lot of different games, but I'm really into competitive online games right now.",
#     "I'm not really into competitive games, I like to play more relaxing games.",
#     "That's cool. What kind of relaxing games do you like to play?",
#     "I like to play puzzle games, simulation games, and story-based games.",
#     "I've never been much of a puzzle game person, but I do like simulation games and story-based games.",
#     "Nice! What's your favorite simulation game?",
#     "I like Stardew Valley a lot. It's a farming game, but it's also really relaxing and fun.",
#     "I've heard good things about that game. I might have to check it out.",
#     "You should! It's a lot of fun.",
#     "Well, I'm glad we met. Maybe we can play some games together sometime.",
#     "That would be fun!"
#     # "Great! I'll send you my Steam game name.",
#     # "Ok, sounds good."
# ]

# conversation = [
#     "Hi there! What's your name?",
#     "Hi! It's (name)",
#     "Nice to meet you, (name). I'm (name)",
#     "Nice to meet you, too",
#     "What do you do for work?",
#     "I'm a nurse",
#     "Oh, that must be a tough job sometimes",
#     "Yeah, it can be. But it's also very rewarding",
#     "What's your favorite thing about your job?",
#     "I love helping people",
#     "That's a great quality to have",
#     "Thank you",
#     "What's your worst moment at work?",
#     "I once lost a patient in my arms",
#     "That must have been very difficult",
#     "It was. But I'm grateful that I was able to be there for him in his final moments",
#     "You're a strong person",
#     "Thank you",
#     "What's your favorite time of day?",
#     "Sunset. It's so peaceful",
#     "I agree. It's a beautiful time of day",
#     "What's your favorite thing to do in your free time?",
#     "I love to read, watch movies, and spend time with my family",
#     "That sounds like a great way to relax",
#     "It is. I love my family very much",
#     "I'm glad",
#     "What's your favorite food?",
#     "I love pizza. What about you?",
#     "I love pizza too!",
#     "Great minds think alike",
#     "I agree",
#     "Do you have any pets?",
#     "I have two dogs",
#     "Oh, that's great! I love dogs.",
#     "Me too. They're the best",
#     "I agree",
#     "I think we're going to be friends",
#     "Me to"
#     ]

# Load Sentence-BERT model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Compute embeddings for each sentence
embeddings = model.encode(conversation, convert_to_tensor=True)

# Compute cosine similarity between consecutive sentences
similarities = util.pytorch_cos_sim(embeddings[:-1], embeddings[1:]).diagonal()

# Detect topic shifts by analyzing significant drops in similarity
threshold = 0.3  # Lower threshold to identify large semantic shifts
topic_shifts = []
for i, similarity in enumerate(similarities):
    if similarity.item() < threshold:  # Use .item() to extract scalar value
        topic_shifts.append((conversation[i], conversation[i + 1]))

# Output topic shifts
topic_shifts

df = synth_tr_turned.iloc[:100,:]

df = df.groupby('conversation_id').apply(lambda group: group[:-2]).reset_index(drop=True)
df.iloc[:20,:]

from sklearn.feature_extraction.text import CountVectorizer
import numpy as np

def compute_keyword_overlap(sentence1, sentence2):
    vectorizer = CountVectorizer(stop_words='english').fit([sentence1, sentence2])
    tokens1 = set(vectorizer.transform([sentence1]).toarray()[0].nonzero()[0])
    tokens2 = set(vectorizer.transform([sentence2]).toarray()[0].nonzero()[0])
    overlap = len(tokens1 & tokens2) / (len(tokens1 | tokens2) + 1e-6)  # Avoid division by zero
    return overlap

from sklearn.feature_extraction.text import CountVectorizer
import spacy

# Load spaCy for lemmatization (optional but can improve results)
nlp = spacy.load('en_core_web_sm')

def compute_keyword_overlap(sentence1, sentence2):
    # Tokenize and lemmatize the sentences
    doc1 = nlp(sentence1)
    doc2 = nlp(sentence2)

    # Extract lemmatized tokens (ignoring stopwords)
    tokens1 = set([token.lemma_ for token in doc1 if not token.is_stop])
    tokens2 = set([token.lemma_ for token in doc2 if not token.is_stop])

    # Compute overlap using Jaccard similarity (intersection over union)
    overlap = len(tokens1 & tokens2) / (len(tokens1 | tokens2) + 1e-6)  # Avoid division by zero
    return overlap

# Initialize a list to store the topic shift results
topic_shifts = []

# Iterate over each conversation
for conversation_id, group in df.groupby('conversation_id'):
    conversation = group['message'].tolist()  # Extract sentences for the current conversation

    # Compute embeddings for each sentence in the conversation
    embeddings = model.encode(conversation, convert_to_tensor=True)

    # Compute cosine similarity between consecutive sentences
    similarities = util.pytorch_cos_sim(embeddings[:-1], embeddings[1:]).diagonal()

    # Detect topic shifts and mark them as True/False
    shifts = []
    sim = []
    key_overlap = []
    threshold = 0.25  # Set a threshold for topic shift detection
    overlap_threshold = 0.4
    for i,similarity in enumerate(similarities):
      keyword_overlap = compute_keyword_overlap(conversation[i], conversation[i+1])

      if similarity.item() < threshold and keyword_overlap < overlap_threshold:  # Detect topic shift
          shifts.append(True)
          #sim.append(similarity)
      else:
          shifts.append(False)
          #sim.append(similarity)

      key_overlap.append(keyword_overlap)


    # Append the topic shift for the last sentence in the conversation (False, as no subsequent sentence exists)
    shifts.append(False)
    sim = similarities.tolist() + [None]
    key_overlap.append(None)


    # Add the topic shifts back to the DataFrame for each conversation's sentences
    df.loc[df['conversation_id'] == conversation_id, 'topic_shift'] = shifts
    df.loc[df['conversation_id'] == conversation_id, 'similarity'] = sim
    df.loc[df['conversation_id'] == conversation_id, 'keyword'] = key_overlap


# Output the updated dataframe with the topic_shift column
#print(df)

df.head(60)

weak_sup= pd.read_csv("/content/drive/MyDrive/602-PrinciplesofDS/Project/Synthetic-PersonaChat/anno_train_all_dialogs.tsv",sep='\t',header=None)
weak_sup.head()

weak_sup.shape



!wget https://github.com/HuiyuanXie/tiage/blob/main/data/personachat/anno/train/anno_train.json



import json
import pandas as pd

# Load the Topical Chat JSON file
with open('/content/anno_train.json', 'r') as f:
    data = json.load(f)

# Lists to store structured conversation data
conversation_id_list = []
sub_conversation_id_list = []
turn_id_list = []
agent_list = []
message_list = []
sentiment_list = []
knowledge_source_list = []
turn_rating_list = []
topic_shift_list = []

# Loop through each conversation in the data
for conversation_id, conversation_data in data.items():
    content = conversation_data['content']

    # Track the last `knowledge_source` to identify shifts
    last_knowledge_source = None
    sub_conversation_id = 0  # Track sub-conversation segments within each main conversation

    # Iterate over the conversation turns
    for i, turn in enumerate(content):
        agent = turn["agent"]
        message = turn["message"]
        sentiment = turn["sentiment"]
        knowledge_source = ', '.join(turn["knowledge_source"])
        turn_rating = turn["turn_rating"]

        # Check if there's a topic shift by comparing knowledge sources
        if last_knowledge_source is not None and knowledge_source != last_knowledge_source:
            sub_conversation_id += 1  # Increment sub-conversation ID on topic shift

        # Set topic shift indicator (True if it's a shift, otherwise False)
        topic_shift = (knowledge_source != last_knowledge_source)

        # Append details to lists
        conversation_id_list.append(conversation_id)
        sub_conversation_id_list.append(sub_conversation_id)
        turn_id_list.append(i)
        agent_list.append(agent)
        message_list.append(message)
        sentiment_list.append(sentiment)
        knowledge_source_list.append(knowledge_source)
        turn_rating_list.append(turn_rating)
        topic_shift_list.append(topic_shift)

        # Update last_knowledge_source for the next turn
        last_knowledge_source = knowledge_source

# Create a DataFrame to organize data
df = pd.DataFrame({
    'conversation_id': conversation_id_list,
    'sub_conversation_id': sub_conversation_id_list,
    'turn_id': turn_id_list,
    'agent': agent_list,
    'message': message_list,
    'sentiment': sentiment_list,
    'knowledge_source': knowledge_source_list,
    'turn_rating': turn_rating_list,
    'topic_shift': topic_shift_list
})

# # Save as CSV or JSONL format as needed
# df.to_csv('processed_conversations_with_topic_shifts.csv', index=False)
# # For JSONL, you can also export it as:
# df.to_json('processed_conversations_with_topic_shifts.jsonl', orient='records', lines=True)

proc_convo2 = pd.read_csv("/content/drive/MyDrive/602-PrinciplesofDS/Project/Synthetic-PersonaChat/processed_conversations2_transformed.csv")

proc_convo2.head()

proc_convo2.shape

proc_convo2.conversation_id.nunique()

proc_convo3 = pd.read_csv("/content/drive/MyDrive/602-PrinciplesofDS/Project/Synthetic-PersonaChat/processed_conversations3_transformed.csv")

proc_convo3.head()

proc_convo3.shape

