# -*- coding: utf-8 -*-
"""data_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K6WJpFER9ToZQdqoAI0zKXK4PuWzqRpY
"""

!wget https://raw.githubusercontent.com/alexa/Topical-Chat/refs/heads/master/conversations/train.json

!wget https://raw.githubusercontent.com/alexa/Topical-Chat/refs/heads/master/conversations/test_freq.json

import json
import pandas as pd

# Load the Topical Chat JSON file
with open('/content/train.json', 'r') as f:
    data = json.load(f)

# Lists to store structured conversation data
conversation_id_list = []
sub_conversation_id_list = []
turn_id_list = []
agent_list = []
message_list = []
sentiment_list = []
knowledge_source_list = []
turn_rating_list = []
topic_shift_list = []

# Loop through each conversation in the data
for conversation_id, conversation_data in data.items():
    content = conversation_data['content']

    # Track the last `knowledge_source` to identify shifts
    last_knowledge_source = None
    sub_conversation_id = 0  # Track sub-conversation segments within each main conversation

    # Iterate over the conversation turns
    for i, turn in enumerate(content):
        agent = turn["agent"]
        message = turn["message"]
        sentiment = turn["sentiment"]
        knowledge_source = ', '.join(turn["knowledge_source"])
        turn_rating = turn["turn_rating"]

        # Check if there's a topic shift by comparing knowledge sources
        if last_knowledge_source is not None and knowledge_source != last_knowledge_source:
            sub_conversation_id += 1  # Increment sub-conversation ID on topic shift

        # Set topic shift indicator (True if it's a shift, otherwise False)
        topic_shift = (knowledge_source != last_knowledge_source)

        # Append details to lists
        conversation_id_list.append(conversation_id)
        sub_conversation_id_list.append(sub_conversation_id)
        turn_id_list.append(i)
        agent_list.append(agent)
        message_list.append(message)
        sentiment_list.append(sentiment)
        knowledge_source_list.append(knowledge_source)
        turn_rating_list.append(turn_rating)
        topic_shift_list.append(topic_shift)

        # Update last_knowledge_source for the next turn
        last_knowledge_source = knowledge_source

# Create a DataFrame to organize data
df = pd.DataFrame({
    'conversation_id': conversation_id_list,
    'sub_conversation_id': sub_conversation_id_list,
    'turn_id': turn_id_list,
    'agent': agent_list,
    'message': message_list,
    'sentiment': sentiment_list,
    'knowledge_source': knowledge_source_list,
    'turn_rating': turn_rating_list,
    'topic_shift': topic_shift_list
})

# # Save as CSV or JSONL format as needed
# df.to_csv('processed_conversations_with_topic_shifts.csv', index=False)
# # For JSONL, you can also export it as:
# df.to_json('processed_conversations_with_topic_shifts.jsonl', orient='records', lines=True)

df.head(20)

# prompt: for the above dataset df, i want to give numerical ids to each conversation, ie numerical conversation_id not this weird long text id....

# Create a mapping from conversation_id to a numerical ID
conversation_id_mapping = {conversation_id: i for i, conversation_id in enumerate(df['conversation_id'].unique())}

# Apply the mapping to create a new 'numerical_conversation_id' column
df['conversation_id'] = df['conversation_id'].map(conversation_id_mapping)

# Display the DataFrame with the new numerical IDs
df.head(100)

final_df = df[['conversation_id', 'sub_conversation_id', 'turn_id', 'agent', 'message', 'topic_shift']]

# prompt: in the above final_df can you confirm that the agent column always has alternating values, as long as the converasation is same.... ie no agent speaks twice??

# Group the DataFrame by conversation_id and sub_conversation_id
grouped = final_df.groupby(['conversation_id', 'sub_conversation_id'])

# Iterate through each group and check if agent values alternate
for (conversation_id, sub_conversation_id), group in grouped:
    agents = group['agent'].tolist()
    for i in range(0, len(agents) - 1, 2):
        if agents[i] == agents[i + 1]:
            print(f"Conversation {conversation_id}, Sub-conversation {sub_conversation_id} has consecutive agent turns.")
            break
    else:
        # print(f"Conversation {conversation_id}, Sub-conversation {sub_conversation_id} has alternating agent turns.")
        pass

final_df.head(20)

import pandas as pd

# Example: Assuming final_df_filtered is your DataFrame
# Group by 'conversation_id' and find the maximum 'sub_conversation_id' for each conversation
max_turns_per_conversation = final_df.groupby('conversation_id')['sub_conversation_id'].max()

# Count the occurrences of each maximum turn count
turn_count_summary = max_turns_per_conversation.value_counts().sort_index()

# Display the summary as a table
turn_count_table = pd.DataFrame({'Max Turns': turn_count_summary.index, 'Number of Conversations': turn_count_summary.values})
print(turn_count_table)

final_df_filtered = final_df[final_df['sub_conversation_id'] <= 10]

# If you want to reset the index after filtering (optional)
final_df_filtered = final_df_filtered.reset_index(drop=True)

# Check if filtering worked as expected
print(f"Original dataset had {final_df.shape[0]} rows.")
print(f"Filtered dataset has {final_df_filtered.shape[0]} rows, with only up to 5 turns per conversation.")

print(final_df_filtered['topic_shift'].value_counts())

final_df_filtered.shape

testdf = final_df_filtered[final_df_filtered['conversation_id']<10]

import pandas as pd


# Analyze shifts
def analyze_topic_shifts(df):
    results = []

    # Group by conversation_id
    for conv_id, group in df.groupby('conversation_id'):
        group = group.reset_index(drop=True)  # Ensure sequential index for processing

        # Identify indices of topic shifts
        shift_indices = group.index[group['topic_shift'] == True].tolist()

        # Calculate the number of rows (turns) between consecutive shifts
        turn_diffs = [shift_indices[i] - shift_indices[i-1] for i in range(1, len(shift_indices))]

        # Measure single-turn vs multi-turn shifts
        single_turn_shifts = sum(1 for diff in turn_diffs if diff == 1)
        multi_turn_shifts = sum(1 for diff in turn_diffs if diff > 1)

        results.append({
            "conversation_id": conv_id,
            "total_shifts": len(shift_indices),
            "single_turn_shifts": single_turn_shifts,
            "multi_turn_shifts": multi_turn_shifts,
            "turn_diffs": turn_diffs,
            "len_turn_diffs":len(turn_diffs)# Detailed differences for analysis
        })

    return pd.DataFrame(results)

# Apply the function
shift_analysis = analyze_topic_shifts(testdf)

# Display the analysis
print(shift_analysis)

shift_analysis[shift_analysis['single_turn_shifts']>=0]

shift_analysis_full = analyze_topic_shifts(final_df_filtered)

shift_analysis_full['len_turn_diffs'].max()

final_df_filtered[final_df_filtered['conversation_id']==6]

import matplotlib.pyplot as plt

# Add a new column to count the number of `1s` in `turn_diffs`
shift_analysis_full['single_turn_ratio'] = shift_analysis_full['turn_diffs'].apply(
    lambda diffs: round(diffs.count(1) / len(diffs),3) if len(diffs) > 0 else 0
)

# Display the updated DataFrame
print(shift_analysis_full[['conversation_id', 'turn_diffs', 'single_turn_ratio']])

# Plot the distribution of the ratio
plt.figure(figsize=(10, 6))
plt.hist(shift_analysis_full['single_turn_ratio'], bins=20, alpha=0.5, color='blue', edgecolor='black')
plt.title("Distribution of Single Turn Shift Ratios", fontsize=10)
plt.xlabel("Ratio of Single Turn Shifts (1s)", fontsize=14)
plt.ylabel("Frequency", fontsize=14)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

shift_analysis_full[shift_analysis_full['single_turn_ratio']<=0.7]

filtered_conversation_ids = shift_analysis_full[
    shift_analysis_full['single_turn_ratio'] <= 0.7
]['conversation_id']

filtered_conversation_ids.shape

filtered_df_str = final_df_filtered[
    final_df_filtered['conversation_id'].isin(filtered_conversation_ids)
]
filtered_df_str

filtered_df_str.conversation_id.nunique()